{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation (German to English)\n",
    "\n",
    "## Content <a name='content'></a>\n",
    "\n",
    "[0. Default Solution](#sec_0)\n",
    "    \n",
    "- [0.1 Qualitative Results on Dev Dataset](#sec_0_1)\n",
    "    \n",
    "- [0.2 Quantitative Results on Dev Dataset](#sec_0_2)\n",
    "    \n",
    "[1. Our Method](#sec_1)\n",
    "\n",
    "- [1.1 Baseline Method](#sec_1_1)\n",
    "    \n",
    "- [1.2 Beam Search](#sec_1_2)\n",
    "    \n",
    "- [1.3 Post Processing: Replication Removal](#sec_1_3)\n",
    "    \n",
    "- [1.4 Post Processing: \\<UNK\\> Replacement](#sec_1_4)\n",
    "    \n",
    "- [1.5 \"Ensemble\" of Models](#sec_1_5)\n",
    "    \n",
    "[2. Experiments](#sec_2)\n",
    "\n",
    "- [2.1 Baseline Method](#sec_2_1)\n",
    "    \n",
    "- [2.2 Beam Search](#sec_2_2)\n",
    "    \n",
    "- [2.3 Replication Removal](#sec_2_3)\n",
    "    \n",
    "- [2.4 \\<UNK\\> Replacement](#sec_2_4)\n",
    "    \n",
    "- [2.5 \"Ensemble\" of Models](#sec_2_5)\n",
    "\n",
    "- [2.6 Final Results](#sec_2_6)\n",
    "    \n",
    "[3.Analysis](#sec_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Default Solution <a name='sec_0'></a> \n",
    "[back to content](#content)\n",
    "\n",
    "The default solution is a seq2seq neural machine translation (NMT) network. The encoder is bi-GRU, and the decoder is GRU. The default solution's problem is that its attention mechanism between the encoder and the decoder is not correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from default import *\n",
    "directory=os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Qualitative Results on Dev Dataset <a name='sec_0_1'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "First, we test the default method. The pre-trained model `seq2seq_E049.pt` is used, and the method is tested on `Dev` dataset. We printed out some translation results as the qualitative results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1305it [03:35,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was , i had my my , i had my . \n",
      "i was in , and i was in . \n",
      "she was a woman . she was a . \n",
      "and she was , and she was , and she was , and she was , and she was , and she was , and she was , and she was , and she was , and she was , and she was . \n",
      "i was , i was , i was . \n",
      "and when i had a first patient , , , a patient . \n",
      "and i was talking about , and i wanted to talk about , and i wanted to talk about . \n",
      "i i i , i i . \n",
      "i did n't have it . \n",
      "so , to the , , , to the , , , to the , , , and , and , and , and , to the \n",
      "i was , , \" , \" , \" , \" , \" , \" \n",
      "later , later , later , later , later came later , later came later later later later . \n",
      "and i had n't think of the , and i had n't have a \n",
      "and i was , to , , , . \n",
      "i i was . . \n",
      "so , you , you , , you , you , you , you , you , you know , you you , you know , you you . \n",
      "and , , \" , , , \" , \" , \" \n",
      "and the 's the to the , the , the , the . \n",
      "that 's what it call . \n",
      "and the was the that that was the that i was the . \n",
      "so , , as a , , , , , , , , , did n't , , \n",
      "and we got the and , and , and , and , and , and we . \n",
      "and , , , , , , , , , , , , , , , , , , , and , and , and , . \n",
      "in the , in the , in the , in the , in the \n",
      "if we 're going to , , , , , , , to say , , and if you 're , , , , you know , \n",
      "if you 're you , you you . \n",
      "i 'll see you a couple of . \n",
      "yeah , yeah , yeah , yeah . \n",
      "so , in the , , , , , , , , , , , , you know , , you know , you can , you know , you know , you . \n",
      "so , the , the , , the , , the , , , the , , , the , , , the , , , the , , the , , the , , the , , the . \n",
      "that 's not that 's . . \n",
      "and the time that we know , that 's that the time that we know . \n",
      "so , , you know , , you , , , , , , , , , , you know , you 're , , , you know , \n",
      "you 're not talking about you 're talking about . \n",
      "i think it 's going to be , , and i think , \n",
      "now , we have a a of the , that we have a a lot of the . \n",
      "we 're not going to be a with the , or the , or the , or the \n",
      "and , the the , that you have to , and that 's what you 're going to do , and that 's what you 're going to \n",
      "and the the , that the the , the the , and the the , and the the the , and the the \n",
      "and so , the the , , and the the . \n",
      "so , in the , we think , we 're in the , we think of the , we think , we 're in the . \n",
      "the fact that the time , the the , the the . \n",
      "and so , there 's a little bit about the there 's a little bit about that there are a . \n",
      "and the people , the people do n't hear the . \n",
      "the the the the the . \n",
      "the 's a a a . \n",
      "and 's the , \" , \" , \" , \" , \" . \n",
      "that 's true . \n",
      "now , the is what we have is the the is what we have . \n",
      "and a little bit of a , and a little bit of a little bit . \n",
      "is n't that ? \n",
      "so , if you you to to , , you , , you , , you know , , you know , , \" \n",
      "nothing . \n",
      "it 's been the the , and it 's the . \n",
      "and i 'm , , and , and i , , and i , and i , and i , and i , and i , and i , and i , and i , and i , \n",
      "i 've been able to say , , \" i , , \" i , \" i , \" \n",
      "and so , , i , , , and i , and i . \n",
      "i had had a , , i had a . . \n",
      "and so , , it 's like , , , it 's like , \" \n",
      "and then , , , , , and , and , and , and , and , and , and then , and , and , and then , \n",
      "i was , i , i , i , i , i , i . \n",
      "what are the people in the ? \n",
      "that does n't do that . \n",
      "so , it 's not , it 's a lot of , , it 's not a lot . \n",
      "if you have a a , , , a , , a , , a , , a a , , a a , , and you can get a \n",
      "and so , the same thing , and the the , and the are the same and and the \n",
      "it 's not going to buy a . \n",
      "you can make it , you can make it . \n",
      "you ca n't want to be able to do that you can not want to be able to do . \n",
      "i 've been thinking , and , and , and , and , and , and , and , and , and i , and , and , and , and , and , and , and \n",
      "what do n't think about what it is to do in the . \n",
      "it 's what it can tell . \n",
      "it 's the story of the . \n",
      "she was she was she was she was she was she was she was she was she was she was . \n",
      "she was n't , in the , , she had been , , and she would have been , , and she would n't have , \n",
      "and she was , with his , , , his , , , his . \n",
      "and , were , and they were , and they were were . \n",
      "they were , , , and they , , , and they , , and they , \" \n",
      "and , , , , , , , , and , and , and . \n",
      "and she was , , and she , and she , and she was , , and she was the , , and she was the , , and she was the , , and she was the . \n",
      "i was going to be , when i was , , and when i was , , \n",
      "i 'm going to have if i 'm going to ? \n",
      "i i , i , i , to say , \" i , to say , \" \n",
      "it was n't really really , really , , \n",
      "i was a , and i was a . \n",
      "so , , , , , , , , , , , , , , , , , it 's \n",
      "and , , , , , , , , , , , , , , and i , , , and , \n",
      "i was n't , if you had a , , , , , i , , i , \n",
      "and i 'd like to to you that you would be . \n",
      "it 's something that you to be something that you 're going to do something . \n",
      "now , i know , i know , , i know , , and i know how to do . \n",
      "so , this is , , this is , , this is , , this is , \n",
      "now , the n't n't have the , but the n't n't know , the n't n't , the , but not the \n",
      "this is a . . \n",
      "and , , , , , but , but , but , but , but . \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "model = Seq2Seq(build=False)\n",
    "model.load(os.path.join('data', 'seq2seq_E049.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# loading test dataset\n",
    "test_iter = loadTestData(os.path.join('data', 'input', 'dev.txt'), model.fields['src'],\n",
    "                            device=device, linesToLoad=sys.maxsize)\n",
    "results = translate(model, test_iter) # Warning: will take >5mins depending on your machine\n",
    "print(\"\\n\".join(results)[:4999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Quantitative Results on Dev Dataset <a name='sec_0_2'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "We evaluated the default method with the `BLEU` metric. The default method obtained a poor result (`BLEU = 3.35`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 3.35 34.4/7.6/2.1/0.6 (BP = 0.771 ratio = 0.794 hyp_len = 19766 ref_len = 24902)\n",
      "3.3529164212375866\n"
     ]
    }
   ],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))\n",
    "print(bleu(ref_t, results).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Our Methods <a name='sec_1'></a>\n",
    "[back to content](#content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Baseline Method <a name='sec_1_1'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "In `baseline method`, we used the same network as the `default method`. The difference is that we fixed the attention mechanism between the encoder and the decoder in `baseline method`.\n",
    "\n",
    "#### Implementation of Attention Mechanism Between Encoder and Decoder\n",
    "\n",
    "#### 1.1.1 Computation of Attention Weights\n",
    "\n",
    "The `AttentionModule` has 3 weight matrices, $W_{enc}$, $W_{dec}$, and $V_{att}$ (which are implemented as the linear layers `nn.Linear`).\n",
    "\n",
    "For each decoder's hidden state $h^{dec}$, we need to compute its relevance score with regard to encoder's outputs $h_{i}^{enc}$. That is, for the next word to translate, we need to find the most relevant information in the encoded representations $h_{i}^{enc}$:\n",
    "\n",
    "$score_{i}=W_{enc}(h_{i}^{enc})+W_{dec}(h^{dec})$\n",
    "\n",
    "Then, we use the `softmax` function to normalized the scores, so that they sum to 1:\n",
    "\n",
    "$\\alpha=softmax(V_{att}tanh(score))$\n",
    "\n",
    "Here, $\\alpha$ is the vector $\\alpha=(\\alpha_{1},...,\\alpha_{n})$ of attention weights. Each $\\alpha_{i}$ describes how the original word at position $i$ (encoded as $h_{i}^{enc}$) is relevant to the next word to translate.\n",
    "\n",
    "The computation of attention weights is implemented in `calcAlpha`.\n",
    "\n",
    "Initially, the tensor $h^{enc}$ (`encoder_out`) and $h^{dec}$ (`decoder_hidden`) are in the shape of `(seq, batch, dim)`. First, we need to transpose the tensors so that they are in the shape of `(batch, seq, dim)`: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "encoder_out=encoder_out.permute(1,0,2)\n",
    "decoder_hidden=decoder_hidden.permute(1,0,2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we implemented the equation:\n",
    "\n",
    "$score_{i}=W_{enc}(h_{i}^{enc})+W_{dec}(h^{dec})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "scores = self.W_enc(encoder_out)+self.W_dec(decoder_hidden)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we implemented the equation:\n",
    "\n",
    "$\\alpha=softmax(V_{att}tanh(score))$\n",
    "\n",
    "to obtain the attention weights $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "scores=self.V_att(torch.tanh(scores))\n",
    "alpha = torch.nn.functional.softmax(scores, dim=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Computation of Context Vector\n",
    "\n",
    "After obtaining the attention weights $\\alpha$, we calculate the weighted sum of $h_{i}^{enc}$ to get the context vector $c$:\n",
    "\n",
    "$c=\\underset{i}{\\Sigma}\\alpha_{i}\\cdot h_{i}^{enc}$\n",
    "\n",
    "This is implemented in the `forward` function of `AttentionModule`.\n",
    "\n",
    "In the `forward` function, we first call the function `calcAlpha` to obtain $\\alpha$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "alpha = self.calcAlpha(decoder_hidden, encoder_out)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we implemented the equation $c=\\underset{i}{\\Sigma}\\alpha_{i}\\cdot h_{i}^{enc}$. Note that initially `encoder_out` ($h^{enc}$) is in the shape of `(seq, batch, dim)`, and we need to transpose it so that it is in the shape of `(batch, seq, dim)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "encoder_out=encoder_out.permute(1,0,2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the transposition, $h_{i}^{enc}$'s index $i$ is in the second dimension (`seq`). To calculate the context vector $c$, we need to take the sum along `dim=1`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "context=torch.sum(alpha*encoder_out, dim=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `context` is in the shape of `(batch, dim)`. In fact, in the implementation the `batch` here is always `1`, so, we can reshape `context` so that it is in the shape of `(1, 1, dim)`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "seq, _, dim = encoder_out.shape\n",
    "context = context.reshape(1, 1, dim)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `alpha` is obtained from `alpha = torch.nn.functional.softmax(scores, dim=1)`, which means that the weights information is in `dim=1`. Now we transpose `alpha` so that the weights information is in `dim=2`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "alpha=alpha.permute(2, 0, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assemble the above implementations in 4 lines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "alpha = self.calcAlpha(decoder_hidden, encoder_out)\n",
    "seq, _, dim = encoder_out.shape\n",
    "context = (torch.sum(alpha*encoder_out.permute(1,0,2), dim=1)).reshape(1, 1, dim)\n",
    "return context, alpha.permute(2, 0, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See experiment results [here](#sec_2_1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Beam Search <a name='sec_1_2'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "In `Beam Search` method, the network architecture and the attention mechanism are the same as in `baseline` method. Here we implemented beam search to allow better decoding strategies for neural machine translation.\n",
    "\n",
    "#### 1.2.1 Overview of the Method\n",
    "\n",
    "Suppose that at time step $t$, given the last translated word $x_{t}$, encoder's output $h^{enc}$, and decoder's hidden state $h_{t}^{dec}$ at time $t$, the decoder returns:\n",
    "\n",
    "$\\tilde{x}_{t+1}, h_{t+1}^{dec}, \\alpha_{t} = Decoder(x_{t}, h^{enc}, h_{t}^{dec})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\tilde{x}_{t+1}$ is a `distribution` of the probability of the next predicted word. For `greedy search` implemented in the `baseline` method, we simply choose the word which maximizes this probability as the next word $x_{t+1}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{t+1}=\\underset{v\\in V}{argmax} \\tilde{x}_{t+1}(v)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in greedy decoding, at each time step $t$, we greedily choose the top one candidate word which gets the highest probability for this time step.\n",
    "\n",
    "In beam search, instead of keeping only the top one candidate word $x_{t}$ at each time step $t$, we keep the top $k$ candidate words, $x_{t,1}, ..., x_{t,k}$, at each time step $t$.\n",
    "\n",
    "So, `greedy search` is a special case of `beam search` with `k=1`. \n",
    "\n",
    "Sometimes, `greedy search` is unable to produce good outcomes, as combining locally optimal choices won't guarantee a globally optimal choice. On the other hand, `exhaustive search` can guarantee the solution's global optimality, however, exhaustively searching over all possibilites is too computationally expensive. So, we want to find a balance between `optimality` and `computational efficiency`, which is why we implemented `beam search`. \n",
    "\n",
    "As the algorithm is not as short-sighted as `greedy search`, with `beam search`, we can get better translations. Meanwhile, we can tune the parameter `k` so that the cost of computational resources is not too much.\n",
    "\n",
    "For each time step $t$, we keep the top $k$ candidate words (the $k$ most probable words), $x_{t,1},...,x_{t,k}$. For each candidate word $x_{t,i}$, we calculate:\n",
    "\n",
    "$\\tilde{x}_{t+1,i}, h_{t+1}^{dec}, \\alpha_{t} = Decoder(x_{t,i}, h^{enc}, h_{t}^{dec})$\n",
    "\n",
    "Here $\\tilde{x}_{t+1,i}$ is a `distribution` of the probability of the next predicted word given $x_{t,i}$ is the word at time step $t$.\n",
    "\n",
    "For each `distribution` $\\tilde{x}_{t+1,i}$, we can find the top $k$ candidate words, $x_{t+1,i,1},...,x_{t+1,i,k}$:\n",
    "\n",
    "$x_{t+1,i,1}=\\underset{v\\in V}{argmax}\\tilde{x}_{t+1,i}(v)$\n",
    "\n",
    "$x_{t+1,i,2}=\\underset{v\\in V-\\{x_{t+1,i,1}\\}}{argmax}\\tilde{x}_{t+1,i}(v)$\n",
    "\n",
    "...\n",
    "\n",
    "$x_{t+1,i,k}=\\underset{v\\in V-\\{x_{t+1,i,1},...,x_{t+1,i,k-1}\\}}{argmax}\\tilde{x}_{t+1,i}(v)$\n",
    "\n",
    "So, for time step $t+1$, we find $k^{2}$ candidate words, $x_{t+1,i,j}(1\\leq i\\leq k, 1\\leq j\\leq k)$. From $x_{t+1,i,j}(1\\leq i\\leq k, 1\\leq j\\leq k)$ we choose the $k$ most probable words as the $k$ candidate words $x_{t+1,1},...,x_{t+1,k}$ for the time step $k+1$.\n",
    "\n",
    "Since $x_{t+1,1},...,x_{t+1,k}$ are generated from $x_{t+1,i,j}(1\\leq i\\leq k, 1\\leq j\\leq k)$, and $x_{t+1,i,j}$s are generated from $x_{t,i}$, of course we can trace back which $x_{t,i}$ generates the $k$ candidate words at time step $t+1$. Hence, at each time step $t$, we can find the $k$ \"so far the most probable\" sequences.\n",
    "\n",
    "We keep generating the $k$ most probable words at each time steps, until we finish the whole sentence. Then, in the $k$ most probable sequences, we pick the most probable one, which is our final translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Overview of Implementation\n",
    "\n",
    "To implement `beam search`, we need to:\n",
    "\n",
    "1) `Initialization`: For the time step $t=1$, find the $k$ most probable words $x_{1,1},...,x_{1,k}$;\n",
    "\n",
    "2) `Recursion`: Given that we have the $k$ most probable sequences at time step $t$, generate the $k^{2}$ candidate words at time step $t+1$. Choose the $k$ most probable words from the $k^{2}$ candidate words.\n",
    "\n",
    "3) `Update`: Given the $k$ most probable words at time step $t+1$, compute the $k$ most probable sequences at time step $t+1$.\n",
    "\n",
    "Then, we can recursively compute the $k$ most probable sequences at any time step, which completes the task of `beam search`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Implementation Detail (I): Auxillary Classes\n",
    "\n",
    "In our implementation, we first define 2 auxillary classes, `Entry` and `Prob_Rank`.\n",
    "\n",
    "`Entry` stores the $k$ most probable sequences so far at time step $t$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class Entry:\n",
    "    def __init__(self,idx_seq,hidden_state_seq,alpha_seq,log_prob):\n",
    "        self.idx_seq=idx_seq # the sequence of words from 1 to t\n",
    "        self.hidden_state_seq=hidden_state_seq # the sequence of hidden states from 1 to t\n",
    "        self.alpha_seq=alpha_seq # the sequence of alpha vectors from 1 to t\n",
    "        self.log_prob=log_prob # logarithm of the probability of the sequence\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Prob_Rank` stores the most probable words at time step $t+1$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class Prob_Rank:\n",
    "    def __init__(self,prev,idx,hidden_state,alpha,log_prob):\n",
    "        self.prev=prev # this word at time step t+1 is generated from which sequence at time step t\n",
    "        self.idx=idx # the generated word at time step t+1\n",
    "        self.hidden_state=hidden_state # hidden state at time step t+1\n",
    "        self.alpha=alpha # alpha vector at time step t+1\n",
    "        self.log_prob=log_prob # logarithm of the joint probability of this word and its previous sequence\n",
    "    def __lt__(self,other):\n",
    "        # we store candidate words at time step t+1 in a min-heap. We use __lt__() to keep the words sorted\n",
    "        # by their corresponding sequences' probability\n",
    "        return self.log_prob<other.log_prob\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Implementation Detail (II): Step 1. Initialization\n",
    "\n",
    "`Beam search` is implemented in the function `greedyDecoder`. First, we provide an additional parameter, `BeamSearchWidth`, for the function `greedyDecoder`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def greedyDecoder(decoder, encoder_out, encoder_hidden, maxLen,\n",
    "                  eos_index, BeamSearchWidth = 25):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `list`, `candidate_list`, to store the top `num_candidate` candidate sequences. Each candidate sequence is represented as an `Entity` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "candidate_list=[]\n",
    "num_candidate=BeamSearchWidth\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we `loop` through the time steps `t`. Recall that our implementation consists of 3 parts:\n",
    "\n",
    "1) For the time step $t=1$, find the $k$ most probable words $x_{1,1},...,x_{1,k}$;\n",
    "\n",
    "2) Given that we have the $k$ most probable sequences at time step $t$, generate the $k^{2}$ candidate words at time step $t+1$. Choose the $k$ most probable words from the $k^{2}$ candidate words.\n",
    "\n",
    "3) Given the $k$ most probable words at time step $t+1$, compute the $k$ most probable sequences at time step $t+1$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `t=0`, we generate the top `num_candidate` most probable words first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for t in range(maxLen):\n",
    "    if t == 0:\n",
    "        # generate the output\n",
    "        output, decoder_hidden, alpha = decoder(output, encoder_out, decoder_hidden)\n",
    "        # \"normalize\" the output as probability distribution\n",
    "        log_prob=torch.log(torch.nn.functional.softmax(output, dim=-1)) \n",
    "        # find the most probable words\n",
    "        candidates=output.data.topk(num_candidate)\n",
    "        # for each of the top candidate words, create an Entity and add it to candidate_list\n",
    "        for i in range(num_candidate):\n",
    "            entry=Entry([],[],[],log_prob[0][0][candidates[1][0][0][i].item()].item())\n",
    "            # out is the token id of the candidate word\n",
    "            out=torch.empty((1,1),dtype=torch.int64)\n",
    "            out[0][0]=candidates[1][0][0][i].item()\n",
    "            entry.idx_seq.append(out)\n",
    "            # store the hidden states in a sequence\n",
    "            entry.hidden_state_seq.append(decoder_hidden)  \n",
    "            # store the alpha vector in a sequence\n",
    "            entry.alpha_seq.append(alpha)\n",
    "            # add the candidate word to the list\n",
    "            candidate_list.append(entry)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 Implementation Detail (III): Step 2. Recursion\n",
    "\n",
    "Then, for `t > 0`, we generate the $k^{2}$ candidate words. We use `Prob_Rank` to represent these $k^{2}$ candidate words. In `Prob_Rank`, we record the candidate word's token id (`Prob_Rank.idx`), the hidden state (`Prob_Rank.hidden_state`), the $\\alpha$ vector (`Prob_Rank.alpha`), the joint probability (`Prob_Rank.log_prob`) of the word and its previous sequence, we also use `Prob_Rank.prev` to keep track of the sequence at the previous time step which generates the current candidate word. We push all `Prob_Rank` objects to a `min Heap` which pops the sub-optimal words out and only stores the top $k$ candidates.\n",
    "\n",
    "Still we need to take special treatment for the sequences which has encountered the `EOS` token in previous time steps. For such sequences, we assume that it has been terminated. If such sequence can stay on the top candidate list, for each time step, we just add a `dummy` Prob_Rank object which doesn't affect the probability estimation of the sequence.\n",
    "\n",
    "For the \"normal\" sequences which haven't encountered the `EOS` token, we need to generate the top $k$ probable words for each of them. To reduce some computational cost, we always store only the top $k$ words as `Prob_Rank` objects in the `min Heap`. When we generate a new word with the `topk` method, we compare its probability with the minimum element in the `min Heap`. If the probability of the new word is greater than the minimum element in the `min Heap`, we pop the minimum element in the `min Heap` out and push the new word to the `min Heap`. If the probability of the new word is no greater than the minimum element in the `min Heap`, then we don't do any thing about the `min Heap`. As we check the words with `descending` likelihood with `topk` method, when we find one word in `topk` list that is not as good as elements in `min Heap`, we can ignore all the rest words in `topk` list.\n",
    "\n",
    "Below is a possible implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# expanded_candidate_list: store the top k words at time step t+1. Organized as min Heap.\n",
    "expanded_candidate_list=[]\n",
    "# next_candidate_list is for the k candidate sequences at time step t+1\n",
    "# (recall that k candidate sequences at time step t are stored in candidate_list)\n",
    "next_candidate_list=[]\n",
    "# store the top k words at time step t+1 in a min Heap. Be ready to pop out the element with minimum probability\n",
    "heapq.heapify(expanded_candidate_list)\n",
    "for i in range(num_candidate):\n",
    "    # we provide special treatment for previous candidate sequences which has been terminated by EOS\n",
    "    if(int(candidate_list[i].idx_seq[-1].data)==eos_index):\n",
    "        # In such cases, we create a dummy node. The joint probability (of this dummy word and the previous \n",
    "        # sequence) is the probability of the previous sequence.\n",
    "        prob_record=Prob_Rank(i,None,None,None,candidate_list[i].log_prob)\n",
    "        # we only store at most num_candidate candidate words in the min Heap.\n",
    "        # if the min Heap is not \"full\", push the word record to the heap\n",
    "        if(len(expanded_candidate_list)<num_candidate):\n",
    "            heapq.heappush(expanded_candidate_list,prob_record)\n",
    "        # if the min Heap is already \"full\" with num_candidate records, we compare the word record's probability\n",
    "        # with the minimum element in the min Heap\n",
    "        else:\n",
    "            # if the word record is sub-optimal, we do nothing about the min Heap.\n",
    "            if (prob_record.log_prob<=min(expanded_candidate_list).log_prob):\n",
    "                continue\n",
    "            # if the word is better than some records in the min Heap, we pop the minimum element out from the \n",
    "            # min Heap, and we push the word into the min Heap\n",
    "            else:\n",
    "                heapq.heappop(expanded_candidate_list)\n",
    "                heapq.heappush(expanded_candidate_list,prob_record)\n",
    "    # for the \"normal\" sequences without an EOS\n",
    "    else:\n",
    "        output, decoder_hidden, alpha = decoder(candidate_list[i].idx_seq[-1], encoder_out, \n",
    "                                                candidate_list[i].hidden_state_seq[-1])\n",
    "        log_prob=torch.log(torch.nn.functional.softmax(output, dim=-1))\n",
    "        # find the top k candidate words from the output distribution\n",
    "        candidates=output.data.topk(num_candidate)\n",
    "        # for each candidate word, create a Prob_Rank record, check if we need to push that to the min Heap\n",
    "        # of the top k words\n",
    "        for j in range(num_candidate):\n",
    "            out=torch.empty((1,1),dtype=torch.int64)\n",
    "            out[0][0]=candidates[1][0][0][j].item()\n",
    "            # keep record of the word token id, hidden states, alpha vector, and update the joint probability\n",
    "            prob_record=Prob_Rank(i,out,decoder_hidden,alpha,candidate_list[i].log_prob+\n",
    "                                log_prob[0][0][candidates[1][0][0][j].item()].item())\n",
    "            # if the min Heap is not \"full\", add the record to the min Heap\n",
    "            if(len(expanded_candidate_list)<num_candidate):\n",
    "                heapq.heappush(expanded_candidate_list,prob_record)\n",
    "            # if the min Heap is \"full\", compare the record with the minimum element in the min Heap\n",
    "            else:\n",
    "                # if the record is sub-optimal, then all the rest records in topk are sub-optimal\n",
    "                # and we need to skip them\n",
    "                if (prob_record.log_prob<=min(expanded_candidate_list).log_prob):\n",
    "                    continue\n",
    "                # if the record is better than some of the records in the min Heap, pop the minimum element\n",
    "                # out, and push the record to the min Heap\n",
    "                else:\n",
    "                    heapq.heappop(expanded_candidate_list)\n",
    "                    heapq.heappush(expanded_candidate_list,prob_record)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.6 Implementation Detail (IV): Step 3. Update\n",
    "\n",
    "So far, we obtained the $k$ most probable words at time step $t+1$. Then, we need to merge them with the list of top $k$ sequences at time step $t$ to create the list of top $k$ sequences at time step $t+1$.\n",
    "\n",
    "So, in the min Heap of the $k$ most probable words (`expanded_candidate_list`), we start from the one with the highest probability (since it's a min Heap, the maximum element is `expanded_candidate_list[-1]`). We use `expanded_candidate_list[ ].prev` to find the corresponding previous sequence at time step $t$ in `candidate_list`. We add the word's token id, hidden state, and alpha vector to the sequence records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# we check the most probable num_candidate words at time step t+1\n",
    "for i in range(num_candidate):\n",
    "    # we start from the most probable word and find its previous sequence: entry\n",
    "    entry=candidate_list[expanded_candidate_list[-i-1].prev]\n",
    "    # we create new_entry, which will store the updated sequence at time step t+1\n",
    "    # we copies entry's (a time step t sequence) sequences of word token ids, hidden states, alpha vectors\n",
    "    # to new_entry\n",
    "    # we update the probability in new_entry\n",
    "    # note that we need to use .copy() method. Or, the append() method will affect both entry and new_entry,\n",
    "    # which creates serious errors for translation\n",
    "    new_entry=Entry(entry.idx_seq.copy(),entry.hidden_state_seq.copy(),\n",
    "                    entry.alpha_seq.copy(),expanded_candidate_list[-i-1].log_prob)\n",
    "    # if the previous sequence has been terminated by EOS, then make no update\n",
    "    # or, we append the word's information to the sequence\n",
    "    if (int(new_entry.idx_seq[-1].data)!=eos_index):\n",
    "        new_entry.idx_seq.append(expanded_candidate_list[-i-1].idx)\n",
    "        new_entry.hidden_state_seq.append(expanded_candidate_list[-i-1].hidden_state)\n",
    "        new_entry.alpha_seq.append(expanded_candidate_list[-i-1].alpha)\n",
    "    # we use next_candidate_list to store the candidate sequences at time step t+1\n",
    "    next_candidate_list.append(new_entry)\n",
    "# now, candidate_list stores the sequences at time step t+1, and next_candidate_list will be emptied\n",
    "# at the beginning of next iteration to store the sequences at time step t+2\n",
    "candidate_list=next_candidate_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.7 Implementation Detail (V): Step 4. Final Output\n",
    "\n",
    "After all iterations, we use the most probable sequence, `candidate_list[0]`, to provide the decoding output for the final translation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for t in range(maxLen):\n",
    "    alphas[t]=candidate_list[0].alpha_seq[t].data\n",
    "    output=candidate_list[0].idx_seq[t]\n",
    "    outputs[t]=torch.zeros(outputs[t].shape)\n",
    "    outputs[t][0][output[0][0].item()]=1\n",
    "    if int(output.data) == eos_index:\n",
    "        break           \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Post Processing: Replication Removal<a name='sec_1_3'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "#### 1.3.1 Why Replication Removal?\n",
    "\n",
    "We noticed that with `baseline method` and `beam search method`, the translation sometimes contains repetitive words, such as \"`when when` i was\". This would negatively affect the `BLEU` score (and translation quality), because the referece translation sentences contain virtually no $n$-gram terms with repetitive words such as `when when`.\n",
    "\n",
    "To improve the `BLEU` score, we proposed to implement the `replication removal` mechanism as a post processing technique in the `translate` function.\n",
    "\n",
    "#### 1.3.2 Problem Formulation\n",
    "\n",
    "For `replication removal`, basically what we are doing is:\n",
    "\n",
    "For the sentence with the sequence of words: $w_{1}, w_{2}, ..., w_{n}$, keep $w_{i}$ ($2\\leq i\\leq n$) in the post-processed sentence *if and only if* $w_{i}\\neq w_{i-1}$.\n",
    "\n",
    "#### 1.3.3 Implementation\n",
    "\n",
    "In `translate` function, after we obtain `output` from the decoder, we perform additional post processing to remove repetitive words. We create a list, `post_processed_output`. A word `w` in the `output` will be added to `post_processed_output`, only if `w` is different from its previous word `w_prev`. Hence, we eliminate all repetitive words in `output` and store the processed sentence in `post_processed_output`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "post_processed_output=[]\n",
    "w_prev=None\n",
    "for w in output.split(\" \"):\n",
    "    if w_prev==None or w_prev!=w:\n",
    "        post_processed_output.append(w)\n",
    "    w_prev=w\n",
    "post_processed_output=\" \".join(post_processed_output)\n",
    "results.append(post_processed_output)\n",
    "return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Post Processing: \\<UNK\\> Replacement <a name='sec_1_4'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "#### 1.4.1 Why \\<UNK\\> Replacement?\n",
    "\n",
    "We realized that the \\<UNK\\> token negatively affects the `BLEU` score, because the reference translation sentences contain no $n$-gram terms with such token.\n",
    "\n",
    "Many \\<UNK\\> replacement strategies can improve `BLEU` score. As \\<UNK\\> has 0 occurrence in reference translation sentences, replacing \\<UNK\\> with any words that has positive occurrence in reference translation sentences might improve `BLEU` score. So, a simple idea is to remove \\<UNK\\>, or replace \\<UNK\\> with some frequent words in English. Such methods can indeed improve `BLEU` score, however they also lead to worsened translation quality. When \\<UNK\\> is removed or replaced by some random words, the readability of the text decreases.\n",
    "\n",
    "To improve `BLEU` score without sacrificing the translation quality, we proposed to replace the \\<UNK\\> token with its corresponding word or words in the original (German) text. This approach has several advantages:\n",
    "\n",
    "1) it doesn't incur additional information loss. The information in the original text is preserved;\n",
    "\n",
    "2) Many German words and English words share the same `etymological sources` (`origin of the words`), so an English reader can correctly infer some of the German words' meaning (for example, the German word `psychotherapie-patientin` is \"psychological therapy patient\");\n",
    "\n",
    "3) Some of the \\<UNK\\> tokens might correspond to `people's names`, or `places`. In such cases, leave the word as it is might be the correct translation.\n",
    "\n",
    "#### 1.4.2 Problem Formulation\n",
    "\n",
    "In the attention mechanism, for each target word, we have computed the alpha vector, which is the target word's relevance to every source words. Now, for the whole sentence with target words $w_{1},...,w_{n}$, we obtained $n$ alpha vectors, $\\alpha_{1},...,\\alpha_{n}$. We can organize all $\\alpha_{1},...,\\alpha_{n}$'s parameters in a matrix $A=(a_{ij})$ so that $a_{ij}$ describes the relevance between the source word $i$ and the target word $j$.\n",
    "\n",
    "Then, for each source word $i$, we compute $p_{i}$, which describes which target word the $i$-th source word is the most relevant to:\n",
    "\n",
    "$p_{i}=\\underset{j}{argmax}\\alpha_{ij}$\n",
    "\n",
    "When we encounter an \\<UNK\\> at position `idx` in the target text, we check all $p_{i}$s to see if there is any $p_{i}=idx$. If so, then the $i$-th source word is the most relevant to the `idx`-th target word and we replace the \\<UNK\\> token in the `idx`-th position by the $i$-th word in the original text.\n",
    "\n",
    "#### 1.4.3 Implementation\n",
    "\n",
    "We implemented \\<UNK\\> replacement in the `translate` function before `replication removal`.\n",
    "\n",
    "First, we added an `additional` parameter, `srcFile`, for the function `translate`, so that the `translate` function can get access to the original German text without tokenization.\n",
    "\n",
    "Then, we obtain the `attention` tensor from the decoder. `attention` is a \\[$batch(=1)$, $L_{source}$, $L_{target}$\\] tensor, which captures how each target word is relevant to which part of the source text (that is, `attention`\\[:, :, i\\] describes the i+1 th target word's distribution of attention in the source text, wheras `attention`\\[:, i, :\\] describes how the i+1 th source word is relevant to each of the target word).\n",
    "\n",
    "We compute `focus` from `attention` by `focus=attention.topk(1)[1]`. Each `focus[i]` describes the location(s) of the target word(s) that the $i$-th source word is most relevant to. When we encounter \\<UNK\\> tokens in the target text, we check `focus` the find out which source word(s) is(are) most relevant to this \\<UNK\\> token. Then, we simply replace the \\<UNK\\> token with the word(s) in the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def translate(model, test_iter, srcFile):\n",
    "    results = []\n",
    "    # srcFile is the additional parameter that we added, so that the translate function can get access to\n",
    "    # source text without tokenization\n",
    "    src = open(srcFile).read().lower().strip().split(\"\\n\")\n",
    "    for i, batch in tqdm(enumerate(test_iter)):\n",
    "        # attention describes relevance between source word i and target word j\n",
    "        output, attention = model(batch.src)\n",
    "        output = output.topk(1)[1]\n",
    "\n",
    "        # focus describes for each source word, which target word(s) is(are) the most relevant\n",
    "        focus=attention.topk(1)[1]\n",
    "\n",
    "        output = model.tgt2txt(output[:, 0].data).strip().split('<EOS>')[0]\n",
    "\n",
    "        for idx, w in enumerate(output.split(\" \")):\n",
    "            # if the idx-th target word is <unk>\n",
    "            if w=='<unk>':\n",
    "                # find out which source words(s) are the most relevant to the idx-th target word\n",
    "                source_word_location=(focus==idx).nonzero(as_tuple=True)\n",
    "                w=''\n",
    "                # replace <unk> with the relevant target word(s) in src (the original text)\n",
    "                for src_idx in range(len(source_word_location[1])):\n",
    "                    w+=src[i].split(\" \")[source_word_location[1][src_idx]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 \"Ensemble\" of Models <a name='sec_1_5'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "#### 1.5.1 Ensemble Decoding\n",
    "\n",
    "According to https://www2.cs.sfu.ca/~anoop/papers/pdf/cmu-lti-dec7-2012.pdf, `ensemble decoding` means that the final probability estimation is made by a `linear interpolation` of the estimations made by each of the models.\n",
    "\n",
    "$p(\\bar{e}|\\bar{f})\\propto exp(\\overset{M}{\\underset{m}{\\Sigma}}\\lambda_{m}log p_{m}(\\bar{e}|\\bar{f}))$\n",
    "\n",
    "Here, we have $M$ models, and $\\lambda_{m} (1\\leq m\\leq M)$ are the weights for the models. Each of the $M$ models made a probability estimation $p_{m}(\\bar{e}|\\bar{f})$, and we calculate the weighted sum:\n",
    "\n",
    "$\\overset{M}{\\underset{m}{\\Sigma}}\\lambda_{m}log p_{m}(\\bar{e}|\\bar{f})$\n",
    "\n",
    "Then, we need to use `softmax` to \"normalize\" the $\\overset{M}{\\underset{m}{\\Sigma}}\\lambda_{m}log p_{m}(\\bar{e}|\\bar{f})$ values into probabilities:\n",
    "\n",
    "$p(\\bar{e}|\\bar{f})=\\frac{exp(\\overset{M}{\\underset{m}{\\Sigma}}\\lambda_{m}log p_{m}(\\bar{e}|\\bar{f}))}{\\underset{\\bar{e}\\in V}{\\Sigma}exp(\\overset{M}{\\underset{m}{\\Sigma}}\\lambda_{m}log p_{m}(\\bar{e}|\\bar{f}))} $\n",
    "\n",
    "If we use this probability estimation function $p(\\bar{e}|\\bar{f})$ for our decoding process, then we are performing `ensemble decoding`.\n",
    "\n",
    "#### 1.5.2 Our \"Ensemble\" of Models\n",
    "\n",
    "In our work, we **didn't** implement the above `ensemble decoding` process. In our implementation, we **didn't** use `ensembling` techniques during `decoding`, instead, we used the \"ensemble\" of models during the `translation` stage.\n",
    "\n",
    "##### Method\n",
    "\n",
    "Assume that we have $M$ models, and each of the $M$ models have generated a decoded sequence $O_{i} (1\\leq i\\leq M)$ and a sequence of attention (alpha) vectors $A_{i} (1\\leq i\\leq M)$. What we are doing is to calculate the linear combination of $O_{i}$s and $A_{i}$s:\n",
    "\n",
    "$O=\\overset{M}{\\underset{i=1}{\\Sigma}}\\lambda_{i}O_{i}, A=\\overset{M}{\\underset{i=1}{\\Sigma}}\\lambda_{i}A_{i}$\n",
    "\n",
    "Here, $\\lambda_{i} (1\\leq i\\leq M)$ is the weights for the model $i$. Instead of using one $O_{i}$ and $A_{i}$ from a single model, we use the linear interpolation $O$ and $A$, where $O$ predicts the target words at each location, and $A$ predicts which part of the source text is relevant to which target word. Since we don't use linear interpolation in the decoding process, we are **not** implementing `ensemble decoding`. However, use did use a linear interpolation of model predictions to guide the `translation` process, which is why we still think that our approach adopts \"ensemble\" of models.\n",
    "\n",
    "##### Implementation\n",
    "\n",
    "Our implementation is as follows.\n",
    "\n",
    "First, in the `main` function, we need to load multiple models and define the `weights`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model1=Seq2Seq(build=False)\n",
    "model1.load(os.path.join('data','seq2seq_E049.pt'))\n",
    "model1.to(hp.device)\n",
    "model1.eval()\n",
    "model2=Seq2Seq(build=False)\n",
    "model2.load(os.path.join('data','seq2seq_E048.pt'))\n",
    "model2.to(hp.device)\n",
    "model2.eval()\n",
    "model3=Seq2Seq(build=False)\n",
    "model3.load(os.path.join('data','seq2seq_E047.pt'))\n",
    "model3.to(hp.device)\n",
    "model3.eval()\n",
    "model4=Seq2Seq(build=False)\n",
    "model4.load(os.path.join('data','seq2seq_E046.pt'))\n",
    "model4.to(hp.device)\n",
    "model4.eval()\n",
    "model5=Seq2Seq(build=False)\n",
    "model5.load(os.path.join('data','seq2seq_E045.pt'))\n",
    "model5.to(hp.device)\n",
    "model5.eval()\n",
    "test_iter=loadTestData(opts.input,model1.fields['src'],device=hp.device,linesToLoad=opts.num)\n",
    "models=[model1,model2,model3,model4,model5]\n",
    "weights=[0.5,0.25,0.125,0.08,0.045]\n",
    "results=translate(models,weights,test_iter,opts.input)\n",
    "print(\"\\n\".join(results))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to modify the parameters of the `translate` function so that it takes an array of models (the variable `models`) and their associated weights (the variable `weights`) as inputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def translate(models, weights, test_iter, srcFile):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we modify the calculation of `output` $O$ and `attention` $A$ in the `translate` function. We wrote a loop to get the `output` and `attention` from each model, and we calculate a linear interpolation with `weights` to get the final `output` and `attention`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "output=None\n",
    "attention=None\n",
    "# \"ensemble\" of models\n",
    "for model, weight in zip(models, weights):\n",
    "    # get each model's output and attention\n",
    "    output_m, attention_m = model(batch.src)\n",
    "    # multiply each model's output and attention with their weights, then add up\n",
    "    if output==None and attention==None:\n",
    "        output=weight*output_m\n",
    "        attention=weight*attention_m\n",
    "    else:\n",
    "        output+=weight*output_m\n",
    "        attention+=weight*attention_m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiments <a name='sec_2'></a>\n",
    "[back to content](#content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Baseline Method <a name='sec_2_1'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "#### 2.1.1 Qualitative Results on Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(directory)\n",
    "from neuralmt_baseline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1305it [04:01,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when when i was in my 20s , i had my first <unk> . . \n",
      "i was a and i was particularly in berkeley . berkeley . \n",
      "she was a a woman woman named alex . \n",
      "when the came came up the first first , she came up and she a a a , and she fell into the couch in my office , <unk> her her and and told me , about about about . \n",
      "and when i heard this , i was was . \n",
      "my my , , had a a patient patient for a first patient . \n",
      "and i got a woman in the people who wanted to talk about about . \n",
      "i 'll leave that , i thought , . \n",
      "but i did n't have it . \n",
      "with the the stories , that alex opened into the , was easy easy to me just to with with head , while we were the problems . \n",
      "\" 30 is the new 20 \" , \" alex , and as far as i did , i had right . \n",
      "work came later later , later came later later , later came later later later later came later later . \n",
      "people people in the the , and alex , and i did n't have time . \n",
      "but soon my my my was , my head , to ask . \n",
      "i i was going to . \n",
      "i said , \" yeah , you know , you 're with with your with your , , you you with a a , but she 's not going to marry . \" \n",
      "and there said my my , \" \" , \" not yet , but maybe you 'll save the next . \n",
      "and also , the best time on the to work before before before they married . \" \n",
      "this is what psychologists call call a call . \n",
      "that was the moment that occurred that i did n't know the new 20 . \n",
      "yes , people people later to look at at times as a , , but did n't mean to do the <unk> . \n",
      "and so , the alex , the , the , and we got there and and and and we sat them . \n",
      "and i realized that this kind of <unk> <unk> <unk> a a problem with real consequences , just just for alex and and their <unk> , but for the , , and , and all the people in the . . \n",
      "at that moment , there in the united states , 50 million people in the . . \n",
      "we talk about about 15 percent of the population , or 100 percent , if you get to get the without the without without going to go through the . . \n",
      "if you guys , if you are in your . . \n",
      "i want to show you some of . \n",
      "oh , yeah ! you 're all incredible . \n",
      "if you work with somebody , the , , , loves , because of the , in the , i 'm going to see you -- -- . . . people in the really are really important . \n",
      "my , my my are in people in the , because i think that every single million million should know what psychologists , and and , the the of of the , and most things that you can do for the work for the , for for happiness , perhaps\n",
      "this is not my opinion . that 's facts . \n",
      "we know that 80 percent of the <unk> moments are to to to 35 . \n",
      "that means that eight of 10 decisions , , experiences and experiences that make out of your life , make it , , until you 're going to be , , \n",
      "do n't you if you 're talking about 40 . \n",
      "this group is going to be fine , i think . \n",
      "we know that the first 10 years of a a a a a a , and how much money money . \n",
      "we know that more than than than half of americans are married or or with the future partner or or partner . \n",
      "we know that brain is going to be the second and big big grow in the the and and and for the the , , which means whatever you want to do , you now time to be doing that . \n",
      "we know that the the are during during the the , when the , in the , and the the the the women the the of the the and the and the and they to 35 things . \n",
      "so in the , , you were <unk> across the body and the the possibilities . \n",
      "when we think about the development of a child , we know all know that the first five years for for years and and are in in brain brain . \n",
      "it 's the time time the the , , life life a impact impact on the future . \n",
      "but we hear about little bit about there there there are something like , and our our are a a time . \n",
      "but the people get n't heard in the the . \n",
      "the press talking about the <unk> of the . . \n",
      "scientists call the the of a a . . \n",
      "journalists are <unk> of the people in people in the the like \" \" or \" \" \" \" \" \" \" . \" \n",
      "that 's true . \n",
      "so , as culture , we have the what the decade decade is the the decade . \n",
      "<unk> <unk> that that things are a with a plan and a little bit little time . \n",
      "is n't that ? \n",
      "so what happens when you put someone in the the the head , and say , \" you you , you you get 10 years to make a little bit of your life ? \n",
      "nothing . \n",
      "this person is the <unk> and the and the and it 's not nothing . \n",
      "and so every day , , people people in the the or or your your and and and say something like , \" i know , my do do n't know , but this relationship is not . i 'm just saying that time . \" \n",
      "or you say , \" all , , as long as i 've done it for 30 years , it 's all all . \" \n",
      "but later , it sounds like , \" my my are almost almost , and i did n't have any . . \n",
      "i had a better fortune when i had finished college . \" \n",
      "and then it sounds like this , in my , , my , like the . \n",
      "all the the and and had fun , but then , 30 , the music came out of and and everybody began to . \n",
      "i did n't want to be the only ones that i stayed , i i sometimes i i , i my husband , because he was in my my in the . chair . \" \n",
      "where are the people in the in here ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "model = Seq2Seq(build=False)\n",
    "model.load(os.path.join('data', 'seq2seq_E049.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# loading test dataset\n",
    "test_iter = loadTestData(os.path.join('data', 'input', 'dev.txt'), model.fields['src'],\n",
    "                            device=device, linesToLoad=sys.maxsize)\n",
    "results = translate(model, test_iter) # Warning: will take >5mins depending on your machine\n",
    "print(\"\\n\".join(results)[:4999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Quantitative Results on Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 17.11 54.0/24.1/12.0/6.5 (BP = 0.957 ratio = 0.958 hyp_len = 23858 ref_len = 24902)\n",
      "17.11393268339238\n"
     ]
    }
   ],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))\n",
    "print(bleu(ref_t, results).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Beam Search <a name='sec_2_2'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "In this experiment, we set the `beam width` parameter as 5.\n",
    "\n",
    "#### 2.2.1 Qualitative Results on Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(directory)\n",
    "from neuralmt_beam_search import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1305it [20:35,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when when i was in my 20s , i had my first <unk> . . \n",
      "i was a , and i studied in berkeley . berkeley . \n",
      "she was a woman woman named alex . \n",
      "as soon as the first arrived came , , she came up and she a a a , and she fell into the couch in my office , <unk> her , and she told me to talk about about <unk> . \n",
      "and when i heard this , i was was . . \n",
      "i mean , my <unk> had had a a patient as a first patient . \n",
      "and i got a woman in the people who wanted to talk about about . . \n",
      "i 'll leave it , i thought i was . \n",
      "but i did n't have it . \n",
      "with the the stories , the alex came into the the , it was easy easy to me , just to head with head , as we faced the problems . . \n",
      "\" 30 is the new 20 \" said , \" alex , and as far as i did , i was right . \n",
      "work came later later , later came later , later came later later , later came later later . \n",
      "people people in the , and alex , and i did n't have time time . \n",
      "but soon , my my , my god was asking in question . \n",
      "i i did it . \n",
      "i said , \" yeah , yeah , you 'll be close with your your , , you you with a a , , but she 's not going to marry it . \" \n",
      "and since said , \" my , \" , \" not yet , but maybe you 'll save the next next . \n",
      "the other thing is the best time on alex to work before before they 're married . \" \n",
      "this is what psychologists call call a <unk> . . \n",
      "that was the moment when i realized realized that 30 was not the new 20 . \n",
      "yes , people people later to look at at the same time , but did did n't know how to do did n't go to . . \n",
      "and this was alex out of the of the , and , and we sat there , and we sat there . \n",
      "and so i realized that this kind of <unk> <unk> was a serious problem with real consequences , just for alex and alex and their <unk> , but , but for the , , and families of all the people in the . . \n",
      "at that moment , in the united states , 50 million people in the . . \n",
      "we talk about about 15 percent of the population , or 100 percent , if you 're not going to achieve the without without going to go through the . . \n",
      "if you guys , when you 're in your . . \n",
      "i i want to show you a couple of . \n",
      "oh , yeah ! you 're all incredible . \n",
      "if you work with somebody in the , , loves loves , because of the , in the , , i 'm going to see you -- -- great . \n",
      "my my are are people in the , , because i believe that every one of these 50 million should know what psychologists , and and wisdom , the one of the most and and most things things you can do for the work for the , for happiness ,\n",
      "this is not my opinion . that 's facts . \n",
      "we know that 80 percent of the <unk> moments are going to happen to 35 . . \n",
      "that means that eight of 10 decisions decisions , experiences and experiences that make out of your life , which make it , until you 're going to be . \n",
      "you do n't <unk> , if you 're talking about 40 . \n",
      "this this group is going to be fine , i think . \n",
      "and we know that the first 10 years of a a of a a have a tremendous amount of how much money is going to deserve . \n",
      "we know that more than than more than half of americans are married or with the future partner or or partner . \n",
      "we know that the brain is going to be and and big big grow in the the , and and for the the , , which means whatever you want to look at the time time to do this . \n",
      "we know that the the are during during the the more than than any of the in the life and and the the of the women women with their peak and and and after 35 are 35 . . \n",
      "so in in the , , you have been <unk> across the body and the the possibilities . \n",
      "when we think about the development of a child , we know all know that the first five years for years and and are in in the brain brain . \n",
      "it 's time time in the ordinary life , life lives a impact impact on the future . \n",
      "but we hear about little bit about there , there 's something like like , , and our time have a a time time . \n",
      "but the people do n't hear in the the . \n",
      "the press talking about the <unk> of the . . \n",
      "scientists call the hackers of a a . . \n",
      "journalists , the of of for people in people in the , like \" <unk> \" \" or \" <unk> . \" \n",
      "that 's true . \n",
      "so as a culture , we have the what 's now the the decade decade for the . \n",
      "<unk> <unk> meant that big things with a plan and a little bit little time . \n",
      "is n't that ? ? \n",
      "so what happens when you put someone in the the ' head , and say say , \" you , you know , you get 10 years to to make some of your life ? \" \n",
      "nothing nothing . \n",
      "this person is the <unk> , and the and it 's not nothing . . \n",
      "and so every every day , interesting people in the people like your your and and daughters into my office and say something like me , \" i do n't know my friend does n't know , but this relationship is not . i 'm just saying that time .\n",
      "or you say , \" all say , as long as i 've done it for 30 years , it 's all fine . \" \n",
      "but later , it sounds like , \" my my is almost almost , and i did n't have anything . . \n",
      "i had a better fortune when i had finished the college . \" \n",
      "and then it sounds like , \" in my my , , , like the . jerusalem . jerusalem . jerusalem . jerusalem . \n",
      "all the <un\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "model = Seq2Seq(build=False)\n",
    "model.load(os.path.join('data', 'seq2seq_E049.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# loading test dataset\n",
    "test_iter = loadTestData(os.path.join('data', 'input', 'dev.txt'), model.fields['src'],\n",
    "                            device=device, linesToLoad=sys.maxsize)\n",
    "results = translate(model, test_iter) # Warning: will take >5mins depending on your machine\n",
    "print(\"\\n\".join(results)[:4999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Quantitative Results on Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 19.04 54.6/25.9/13.5/7.6 (BP = 0.978 ratio = 0.978 hyp_len = 24354 ref_len = 24902)\n",
      "19.03769045192745\n"
     ]
    }
   ],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))\n",
    "print(bleu(ref_t, results).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Replication Removal <a name='sec_2_3'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "In this experiment, we tested the combination of `beam search` and `replication removal`. The `beam width` parameter is set as 5.\n",
    "\n",
    "#### 2.3.1 Qualitative Results on Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(directory)\n",
    "from neuralmt_beam_search_with_replication_removal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1305it [20:14,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when i was in my 20s , i had my first <unk> . \n",
      "i was a , and i studied in berkeley . berkeley . \n",
      "she was a woman named alex . \n",
      "as soon as the first arrived came , she came up and she a , and she fell into the couch in my office , <unk> her , and she told me to talk about <unk> . \n",
      "and when i heard this , i was . \n",
      "i mean , my <unk> had a patient as a first patient . \n",
      "and i got a woman in the people who wanted to talk about . \n",
      "i 'll leave it , i thought i was . \n",
      "but i did n't have it . \n",
      "with the stories , the alex came into the , it was easy to me , just to head with head , as we faced the problems . \n",
      "\" 30 is the new 20 \" said , \" alex , and as far as i did , i was right . \n",
      "work came later , later came later , later came later , later came later . \n",
      "people in the , and alex , and i did n't have time . \n",
      "but soon , my , my god was asking in question . \n",
      "i did it . \n",
      "i said , \" yeah , yeah , you 'll be close with your , you with a , but she 's not going to marry it . \" \n",
      "and since said , \" my , \" , \" not yet , but maybe you 'll save the next . \n",
      "the other thing is the best time on alex to work before they 're married . \" \n",
      "this is what psychologists call a <unk> . \n",
      "that was the moment when i realized that 30 was not the new 20 . \n",
      "yes , people later to look at the same time , but did n't know how to do did n't go to . \n",
      "and this was alex out of the of the , and , and we sat there , and we sat there . \n",
      "and so i realized that this kind of <unk> was a serious problem with real consequences , just for alex and alex and their <unk> , but , but for the , and families of all the people in the . \n",
      "at that moment , in the united states , 50 million people in the . \n",
      "we talk about 15 percent of the population , or 100 percent , if you 're not going to achieve the without going to go through the . \n",
      "if you guys , when you 're in your . \n",
      "i want to show you a couple of . \n",
      "oh , yeah ! you 're all incredible . \n",
      "if you work with somebody in the , loves , because of the , in the , i 'm going to see you -- great . \n",
      "my are people in the , because i believe that every one of these 50 million should know what psychologists , and wisdom , the one of the most and most things you can do for the work for the , for happiness ,\n",
      "this is not my opinion . that 's facts . \n",
      "we know that 80 percent of the <unk> moments are going to happen to 35 . \n",
      "that means that eight of 10 decisions , experiences and experiences that make out of your life , which make it , until you 're going to be . \n",
      "you do n't <unk> , if you 're talking about 40 . \n",
      "this group is going to be fine , i think . \n",
      "and we know that the first 10 years of a of a have a tremendous amount of how much money is going to deserve . \n",
      "we know that more than more than half of americans are married or with the future partner or partner . \n",
      "we know that the brain is going to be and big grow in the , and for the , which means whatever you want to look at the time to do this . \n",
      "we know that the are during the more than any of the in the life and the of the women with their peak and after 35 are 35 . \n",
      "so in the , you have been <unk> across the body and the possibilities . \n",
      "when we think about the development of a child , we know all know that the first five years for years and are in the brain . \n",
      "it 's time in the ordinary life , life lives a impact on the future . \n",
      "but we hear about little bit about there , there 's something like , and our time have a time . \n",
      "but the people do n't hear in the . \n",
      "the press talking about the <unk> of the . \n",
      "scientists call the hackers of a . \n",
      "journalists , the of for people in people in the , like \" <unk> \" or \" <unk> . \" \n",
      "that 's true . \n",
      "so as a culture , we have the what 's now the decade for the . \n",
      "<unk> meant that big things with a plan and a little bit little time . \n",
      "is n't that ? \n",
      "so what happens when you put someone in the ' head , and say , \" you , you know , you get 10 years to make some of your life ? \" \n",
      "nothing . \n",
      "this person is the <unk> , and the and it 's not nothing . \n",
      "and so every day , interesting people in the people like your and daughters into my office and say something like me , \" i do n't know my friend does n't know , but this relationship is not . i 'm just saying that time .\n",
      "or you say , \" all say , as long as i 've done it for 30 years , it 's all fine . \" \n",
      "but later , it sounds like , \" my is almost , and i did n't have anything . \n",
      "i had a better fortune when i had finished the college . \" \n",
      "and then it sounds like , \" in my , like the . jerusalem . jerusalem . jerusalem . jerusalem . \n",
      "all the <unk> and had fun , but then with 30 , the music came out of and everybody started to . \n",
      "i did n't want to be the only ones that i stayed . i sometimes , i , i my husband , because he was in my chair . \" chair . \" \n",
      "where are the people in the in here ? \n",
      "does n't do that . \n",
      "ok , that 's easy to say , but it does n't mistake , because it 's about much . \n",
      "if a lot of things in the , there 's a enormous pressure to go back to a career , a city , a city , a partne\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "model = Seq2Seq(build=False)\n",
    "model.load(os.path.join('data', 'seq2seq_E049.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# loading test dataset\n",
    "test_iter = loadTestData(os.path.join('data', 'input', 'dev.txt'), model.fields['src'],\n",
    "                            device=device, linesToLoad=sys.maxsize)\n",
    "results = translate(model, test_iter) # Warning: will take >5mins depending on your machine\n",
    "print(\"\\n\".join(results)[:4999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Quantitative Results on Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 19.39 60.6/29.2/15.8/9.1 (BP = 0.863 ratio = 0.872 hyp_len = 21715 ref_len = 24902)\n",
      "19.386790425063992\n"
     ]
    }
   ],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))\n",
    "print(bleu(ref_t, results).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 \\<UNK\\> Replacement <a name='sec_2_4'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "In this experiment, we tested the combination of `beam search`, `replication removal`, and `<UNK> replacement`. The `beam width` parameter is set as 5.\n",
    "\n",
    "#### 2.4.1 Qualitative Results on Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(directory)\n",
    "from neuralmt_beam_search_with_replication_removal_with_unk_replacement import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1305it [20:09,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when i was in my 20s , i had my first psychotherapie-patientin . \n",
      "i was a , and i studied in berkeley . berkeley . \n",
      "she was a woman named alex . \n",
      "as soon as the first arrived came , she came up and she a , and she fell into the couch in my office , schleuderte her , and she told me to talk about  . \n",
      "and when i heard this , i was . \n",
      "i mean , my kommilitonin had a patient as a first patient . \n",
      "and i got a woman in the people who wanted to talk about . \n",
      "i 'll leave it , i thought i was . \n",
      "but i did n't have it . \n",
      "with the stories , the alex came into the , it was easy to me , just to head with head , as we faced the problems . \n",
      "\" 30 is the new 20 \" said , \" alex , and as far as i did , i was right . \n",
      "work came later , later came later , later came later , later came later . \n",
      "people in the , and alex , and i did n't have time . \n",
      "but soon , my , my god was asking in question . \n",
      "i did it . \n",
      "i said , \" yeah , yeah , you 'll be close with your , you with a , but she 's not going to marry it . \" \n",
      "and since said , \" my , \" , \" not yet , but maybe you 'll save the next . \n",
      "the other thing is the best time on alex to work before they 're married . \" \n",
      "this is what psychologists call a aha-moment . \n",
      "that was the moment when i realized that 30 was not the new 20 . \n",
      "yes , people later to look at the same time , but did n't know how to do did n't go to . \n",
      "and this was alex out of the of the , and , and we sat there , and we sat there . \n",
      "and so i realized that this kind of harmloser vernachlssigung was a serious problem with real consequences , just for alex and alex and their liebesleben , but , but for the , and families of all the people in the . \n",
      "at that moment , in the united states , 50 million people in the . \n",
      "we talk about 15 percent of the population , or 100 percent , if you 're not going to achieve the without going to go through the . \n",
      "if you guys , when you 're in your . \n",
      "i want to show you a couple of . \n",
      "oh , yeah ! you 're all incredible . \n",
      "if you work with somebody in the , loves , because of the , in the , i 'm going to see you -- great . \n",
      "my are people in the , because i believe that every one of these 50 million should know what psychologists , and wisdom , the one of the most and most things you can do for the work for the , for happiness ,\n",
      "this is not my opinion . that 's facts . \n",
      "we know that 80 percent of the prgendsten moments are going to happen to 35 . \n",
      "that means that eight of 10 decisions , experiences and experiences that make out of your life , which make it , until you 're going to be . \n",
      "you do n't panisch , if you 're talking about 40 . \n",
      "this group is going to be fine , i think . \n",
      "and we know that the first 10 years of a of a have a tremendous amount of how much money is going to deserve . \n",
      "we know that more than more than half of americans are married or with the future partner or partner . \n",
      "we know that the brain is going to be and big grow in the , and for the , which means whatever you want to look at the time to do this . \n",
      "we know that the are during the more than any of the in the life and the of the women with their peak and after 35 are 35 . \n",
      "so in the , you have been weiterbilden across the body and the possibilities . \n",
      "when we think about the development of a child , we know all know that the first five years for years and are in the brain . \n",
      "it 's time in the ordinary life , life lives a impact on the future . \n",
      "but we hear about little bit about there , there 's something like , and our time have a time . \n",
      "but the people do n't hear in the . \n",
      "the press talking about the zeitverschiebung of the . \n",
      "scientists call the hackers of a . \n",
      "journalists , the of for people in people in the , like \" twixters \" or \" \"kidults . \" \n",
      "that 's true . \n",
      "so as a culture , we have the what 's now the decade for the . \n",
      "leonard bernstein meant that big things with a plan and a little bit little time . \n",
      "is n't that ? \n",
      "so what happens when you put someone in the ' head , and say , \" you , you know , you get 10 years to make some of your life ? \" \n",
      "nothing . \n",
      "this person is the ambition , and the and it 's not nothing . \n",
      "and so every day , interesting people in the people like your and daughters into my office and say something like me , \" i do n't know my friend does n't know , but this relationship is not . i 'm just saying that time .\n",
      "or you say , \" all say , as long as i 've done it for 30 years , it 's all fine . \" \n",
      "but later , it sounds like , \" my is almost , and i did n't have anything . \n",
      "i had a better fortune when i had finished the college . \" \n",
      "and then it sounds like , \" in my , like the . jerusalem . jerusalem . jerusalem . jerusalem . \n",
      "all the liefen and had fun , but then with 30 , the music came out of and everybody started to . \n",
      "i did n't want to be the only ones that i stayed . i sometimes , i , i my husband , because he was in my chair . \" chair . \" \n",
      "where are the people in the in here ? \n",
      "does n't do that . \n",
      "ok , that 's easy to say , but it does n't mistake , because it 's about much . \n",
      "i\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "model = Seq2Seq(build=False)\n",
    "model.load(os.path.join('data', 'seq2seq_E049.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# loading test dataset\n",
    "test_iter = loadTestData(os.path.join('data', 'input', 'dev.txt'), model.fields['src'],\n",
    "                            device=device, linesToLoad=sys.maxsize)\n",
    "results = translate(model, test_iter, os.path.join('data', 'input', 'dev.txt')) # Warning: will take >5mins depending on your machine\n",
    "print(\"\\n\".join(results)[:4999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Quantitative Results on Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 19.71 61.1/29.7/16.2/9.3 (BP = 0.862 ratio = 0.870 hyp_len = 21675 ref_len = 24902)\n",
      "19.712473363165547\n"
     ]
    }
   ],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))\n",
    "print(bleu(ref_t, results).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 \"Ensemble\" of Models <a name='sec_2_5'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "In this experiment, we tested the combination of `beam search`, `replication removal`, `<UNK> replacement`, and `ensemble of models`. The `beam width` parameter is set as 5.\n",
    "\n",
    "#### 2.5.1 Qualitative Results on Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(directory)\n",
    "from neuralmt_ensemble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1305it [2:41:14,  7.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when i was in my 20s , i had my first psychotherapie-patientin . \n",
      "i was a , and i studied in berkeley . berkeley . \n",
      "she was a woman named alex . \n",
      "as soon as the first arrived came , she came up and she a , and she fell into the office in my office , schleuderte her , and she told me to talk about zu . \n",
      "and when i heard this , i was . \n",
      "i mean , my kommilitonin had a patient as a first patient . \n",
      "and i got a woman in the people who wanted to talk about . \n",
      "i 'll leave it , i thought i was . \n",
      "but i did n't have it . \n",
      "with the stories , the alex came into the , it was easy to me , to with head , we faced the problems . \n",
      "\" 30 is the new 20 \" , \" , and as i , i was right . \n",
      "work came later , later came later , later came later , later came later . \n",
      "people in the , and alex , and i did n't have time . \n",
      "but soon , my , my god was asking in question . \n",
      "i did it . \n",
      "i said , \" yeah , yeah , you 'll be close with your , you with a , but she 's not going to marry it . \" \n",
      "and since said , \" , \" not yet , but maybe you 'll save the next . \n",
      "the other thing is the best time on alex to work before they 're married . \" \n",
      "this is what psychologists call a aha-moment . \n",
      "that was the moment when i realized that 30 was not the new 20 . \n",
      "yes , people later to look at the same time , but did n't know how to do did n't go to \n",
      "and this was alex out of the of the , and , and we sat there , and we sat there \n",
      "and so i realized that this kind of harmloser vernachlssigung was a serious problem with real consequences , just for alex and alex and their liebesleben , but , but for , and families of all the people in the . \n",
      "at that moment , in the united states , 50 million people in the . \n",
      "we talk about 15 percent of the population , or 100 percent , if you 're not going to achieve the without going to go through the . \n",
      "if you guys , when you 're in your . \n",
      "i want to show you a couple of . \n",
      "oh , yeah ! you 're all incredible . \n",
      "if you work with somebody in the , loves , because of the , in the , i 'm going to see you -- great . \n",
      "my are people in the , because i believe that every one of these 50 million should know what psychologists , and wisdom , the one of the most and things you can do for the work for the , for happiness ,\n",
      "this is not my opinion . that is facts . \n",
      "we know that 80 percent of the prgendsten moments are going to happen to 35 . \n",
      "that means that eight of 10 decisions , experiences and experiences that make out of your life , which make it , until you 're going to be . \n",
      "you do n't panisch , if you 're talking about 40 \n",
      "this group is going to be fine , i think . \n",
      "and we know that the first 10 years of a of a have a tremendous amount of how much money is going to deserve . \n",
      "we know that more than more than half of americans are married or with the future partner or partner . \n",
      "we know that the brain is going to be and big grow in the , and for the , which means whatever you want to look at the time to do this . \n",
      "we know that the are during the more than any of the in the life and the of the women with their peak and after 35 are 35 . \n",
      "so in the , you have been weiterbilden across the body and the possibilities . \n",
      "when we think about the development of a child , we know all know that the first five for and in the brain . \n",
      "it 's time in the ordinary , life lives a impact on the future . \n",
      "but we hear about little bit about there , there 's something like , and our time have a time . \n",
      "but the people do n't hear in the . \n",
      "the press talking about the zeitverschiebung of the  . \n",
      "scientists call the hackers of a . \n",
      "journalists , the of for people in people in the , like \" twixters \" or \"  . \" \n",
      "that 's true . \n",
      "so as a culture , we have the what 's now the decade for the . \n",
      "leonard bernstein meant that big things with a plan and a little bit of time . \n",
      "is n't that ? \n",
      "so what happens when you put someone in the ' head , and say , \" you , you know , you get 10 years to make some of your life \n",
      "nothing . \n",
      "this person is the ambition , and the and the 's not nothing . \n",
      "and so every , people in the people like your and daughters into my office and say something like me , \" i do n't know my friend does n't know , but this relationship is not . i 'm just saying that time .\n",
      "or you say , \" all say , as long as i 've done it for 30 years , it 's all fine . \" \n",
      "but later , it sounds like , \" my is almost , and i did n't have anything . \n",
      "i had a better fortune when i had finished the college . \" \n",
      "and then it sounds like , \" in my , like the . jerusalem . jerusalem . jerusalem . \n",
      "all the  and had fun , but then with 30 , the came out of and everybody started to . \n",
      "i did n't want to be the only ones that i stayed . i sometimes , i , i my husband , because he was in my chair . \" chair . \" \n",
      "where are the people in the in here ? \n",
      "does n't do that . \n",
      "ok , that 's easy to say , but it does n't mistake , because it 's about much . \n",
      "if a lot of things in the , there 's a enormous pressure to go back to a career , a city , a city , a partner and two or three or\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "model1=Seq2Seq(build=False)\n",
    "model1.load(os.path.join('data','seq2seq_E049.pt'))\n",
    "model1.to(hp.device)\n",
    "model1.eval()\n",
    "model2=Seq2Seq(build=False)\n",
    "model2.load(os.path.join('data','seq2seq_E048.pt'))\n",
    "model2.to(hp.device)\n",
    "model2.eval()\n",
    "model3=Seq2Seq(build=False)\n",
    "model3.load(os.path.join('data','seq2seq_E047.pt'))\n",
    "model3.to(hp.device)\n",
    "model3.eval()\n",
    "model4=Seq2Seq(build=False)\n",
    "model4.load(os.path.join('data','seq2seq_E046.pt'))\n",
    "model4.to(hp.device)\n",
    "model4.eval()\n",
    "model5=Seq2Seq(build=False)\n",
    "model5.load(os.path.join('data','seq2seq_E045.pt'))\n",
    "model5.to(hp.device)\n",
    "model5.eval()\n",
    "test_iter=loadTestData(os.path.join('data', 'input', 'dev.txt'),model1.fields['src'],device=hp.device,linesToLoad=sys.maxsize)\n",
    "models=[model1,model2,model3,model4,model5]\n",
    "weights=[0.5,0.25,0.125,0.08,0.045]\n",
    "results=translate(models,weights,test_iter,os.path.join('data', 'input', 'dev.txt'))\n",
    "print(\"\\n\".join(results)[:4999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Quantitative Results on Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 19.23 61.3/29.5/15.9/9.2 (BP = 0.848 ratio = 0.859 hyp_len = 21384 ref_len = 24902)\n",
      "19.227925009919375\n"
     ]
    }
   ],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))\n",
    "print(bleu(ref_t, results).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Final Results <a name='sec_2_6'></a>\n",
    "[back to content](#content)\n",
    "\n",
    "In this experiment, we reported our `best-so-far` result. We used a combination of `beam search`, `replication removal`, and `<UNK> replacement`. The `beam width` parameter is set as 25.\n",
    "\n",
    "#### 2.6.1 Qualitative Results on Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(directory)\n",
    "from neuralmt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1305it [1:49:06,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when i was in my 20s , i had my first psychotherapie-patientin . \n",
      "i was a , and i studied in berkeley . berkeley . \n",
      "she was a woman named alex . \n",
      "as soon as the first arrived came , she came up , and she was a , and she fell into the street in my office , schleuderte her , and she told me to talk about  . \n",
      "and when i heard this , i was  . \n",
      "i mean , my kommilitonin had a patient as a first patient . \n",
      "and i got a woman in the people who wanted to talk about . \n",
      "i 'll leave it , i thought . \n",
      "but i did n't have it . \n",
      "with the stories , the alex came into the , it was easy to me , just to deal with head , as we faced the problems . \n",
      "\" 30 is the new 20 \" , \" alex , and as far as i did , they had right . \n",
      "work came later , later came later , later came later , later came later . \n",
      "people in the , like alex , and i did n't have time . \n",
      "but soon , my , my god was asking in question . \n",
      "i was holding . \n",
      "i said , \" yeah , yeah , you 'll be close with your , you with a , but she 's not going to marry it . \" \n",
      "and he said , \" well , \" not yet , but maybe you 'll save the next . \n",
      "another thing is the best time on alex to work before they married . \" \n",
      "this is what psychologists call a aha-moment . \n",
      "that was the moment when i realized that 30 was not the new 20 . \n",
      "yes , people used to look at the times , but did n't mean any of the . \n",
      "and this was alex at the moment of the , and we sat there , and we sat there . \n",
      "and then i realized that this kind of harmloser vernachlssigung was a serious problem with real consequences , just for alex and their liebesleben , but for the , and families of all the people in the . \n",
      "at that moment , in the united states , 50 million people in the . \n",
      "we talk about 15 percent of the population , or 100 percent if you 're not going to achieve the without going to go through the . \n",
      "thank you guys when you 're in your . \n",
      "i want to do a couple of you . \n",
      "oh , yeah ! you 're all incredible . \n",
      "if you work with somebody in the , one loves , because of the in the , i 'd like to see you -- . people in the really are really important . \n",
      "my are people in the , because i believe that every one of these 50 million should know what psychologists , and wisdom , the one of the most and most things that you can do for the work for the , for happiness ,\n",
      "this is not my opinion . that 's facts . \n",
      "we know that 80 percent of the prgendsten moments are going to happen to 35 . \n",
      "that means that eight of 10 decisions , experiences and experiences that make out of your life , which make it , until you 're going to be . \n",
      "do n't want panisch if you 're talking about 40 . \n",
      "this group is going to be fine , i think . \n",
      "now , we know that the first 10 years of a is a tremendous impact on how much money is going to deserve . \n",
      "we know that more than more than half of americans are married or with the future partner or partner . \n",
      "we know that the brain is going to be and big grow in the , and for the , which means whatever you want to look at the time to do this . \n",
      "we know that the citizens are during the more than the same time in the life and the of the women with their peak and after 35 are 35 . \n",
      "so in the ' , you should be weiterbilden across the body and your own possibilities . \n",
      "when we think about the development of a child , we know all know that the first five years for drugs and are in the brain . \n",
      "it 's the time when the ordinary , life has a impact on the future . \n",
      "but we hear about little bit about there , there 's something like , and our time are a time . \n",
      "but the people do n't hear in the . \n",
      "the press talking about the zeitverschiebung of the  . \n",
      "scientists call the hackers of a . \n",
      "journalists is the stupid of people in the , like \" twixters \" or \" kidults . \" \n",
      "that 's true . \n",
      "so as a culture , we have the what 's now the decade for the food . \n",
      "leonard bernstein meant that big things with a plan and a little bit of time . \n",
      "is n't that ? \n",
      "so what happens when you put someone in the ' head , and say , \" you , you know , you get 10 years to make to make some of your life ? \" \n",
      "nothing . \n",
      "this person is the ambition , and the and it 's not nothing . \n",
      "and so every day , interesting people in the people like your sons and daughters into my office and say something like me , \" i do n't know my friend does n't know , but that relationship is n't . i 'm just saying that time .\n",
      "or you say , \" all say , as long as i 've done it for 30 years , it 's all good . \" \n",
      "but later , it sounds like , \" my is almost , and i did n't have anything . \n",
      "i had a better fortune when i had finished the college . \" \n",
      "and then it sounds like , \" in my 20s , partnersuche like the journey . jerusalem . jerusalem . \n",
      "all the liefen and they had fun , but then with 30 , the music came out of and everybody began to . \n",
      "i did n't want to be the only ones that i stayed , and i sometimes , i did my husband , because he was in my in the my chair . \" chair . \" \n",
      "where are the people in the in here ? \n",
      "does n't do that . \n",
      "ok , that 's easy to say , but it does n't \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "model = Seq2Seq(build=False)\n",
    "model.load(os.path.join('data', 'seq2seq_E049.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# loading test dataset\n",
    "test_iter = loadTestData(os.path.join('data', 'input', 'dev.txt'), model.fields['src'],\n",
    "                            device=device, linesToLoad=sys.maxsize)\n",
    "results = translate(model, test_iter, os.path.join('data', 'input', 'dev.txt')) # Warning: will take >5mins depending on your machine\n",
    "print(\"\\n\".join(results)[:4999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2 Quantitative Results on Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 20.31 61.0/30.0/16.6/9.7 (BP = 0.871 ratio = 0.878 hyp_len = 21875 ref_len = 24902)\n",
      "20.30719054449544\n"
     ]
    }
   ],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))\n",
    "print(bleu(ref_t, results).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis <a name='sec_3'></a>\n",
    "[back to content](#content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Attention is Crucial for Neural Machine Translation\n",
    "\n",
    "From both qualitative and quantitative results we can confirm that attention mechanism greatly improves the\n",
    "translation quality measured either by BLEU scores or human evaluation. Without attention mechanism, the default\n",
    "method can still make correct translations for some of the words in the sentence (for example, the default method\n",
    "correctly translated I was in the sentence When I was in my 20s , and it correctly translated she was a woman\n",
    "in the sentence She was a 26-year-old woman named Alex). However, without attention, the default method seems\n",
    "to only be able to recognize some of the most frequent words or phrases in English, and it makes a lot of repetitive\n",
    "translations. The rich information in the sentence, is not attended to by the model and lost. From such results we can\n",
    "conclude that, without attention, the encoders last hidden state is a bottleneck of information, which cant provide a\n",
    "good summary of the source sentence. With attention mechanism, the model can attend to the most relevant part in\n",
    "the source text for the translation of each target words, which greatly improves the quality of neural machine\n",
    "translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Larger Beam Search Width is Not Always Good\n",
    "\n",
    "From the quantitative results we can see that the BLEU score peaks when the beam width is around $k$ = 25. After the\n",
    "beam search width reaches $k$ = 25, further increasing the beam search width helps little for improving BLEU scores.\n",
    "In fact, it is a little bit counter-intuitive that increasing $k$ from 25 to 30 actually leads to lower BLEU scores. Our\n",
    "preliminary explanation for this is that, while increase the beam search width ensures that the algorithm can search\n",
    "over a larger space to find the optimal solution, the optimality defined by the training dataset might be different\n",
    "from the optimality required in the testing set. As we increase the beam search width, the algorithm makes a more\n",
    "thorough search, and the solution might converge to the optimality according to the training set, however, this doesnt\n",
    "guarantee that the solution is optimal for the testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Translation Quality is Different From BLEU Scores\n",
    "\n",
    "If the translated text contains some tokens or patterns of tokens which would never appear in the reference translation\n",
    "text, then such tokens or patterns of tokens would negatively affect the BLEU score. Removing such tokens, or\n",
    "replacing such tokens with some frequent words in English, can always be helpful for improving the BLEU score.\n",
    "However, such technique might actually leads to worsened translation quality, because the text can become confusing\n",
    "after removal of tokens or replacement of the tokens with some random words. What we learned from the experiments\n",
    "is that we still want to replace such tokens with some relevant information, so that we can preserve the information in\n",
    "the text, and improve the BLEU score without sacrificing the translation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Why Ensemble Fails?\n",
    "\n",
    "In our experiments, we tested the ensemble of models, however, the result of ensemble is not as good as the same\n",
    "method without ensemble. We have 2 preliminary explanations for this: 1) our implementation of ensemble is not\n",
    "good, and its result is still far from the results that we can get from ensemble decoding. 2) we guess that we need to\n",
    "use a number of fundamentally different models, so that the ensemble of models can produce a good results. In our\n",
    "experiments, all models are using the same network architecture, and we guess that their only possible differences are\n",
    "the training epochs and parameters. Since all models use the same network architecture, they share the same\n",
    "advantages and drawbacks. Since their only difference might be the training epochs, maybe in the 5 models, some are\n",
    "just better than others because they are trained with more epochs. We guess that in ensemble learning, we actually\n",
    "want the models to have different advantages, so that they can cover each others drawbacks. In our experiments, all\n",
    "5 models are homogenous with the same network architecture, same advantages, and same drawbacks, which might\n",
    "be a reason why ensemble of such models wont produce good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
